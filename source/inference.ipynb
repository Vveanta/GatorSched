{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxyHLw2EgcKn"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and mount Google Drive\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbpQp-zsnOzs",
        "outputId": "1770ac54-6dc8-40c4-ea64-56a24f7e208d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnrJZ_-ng6T",
        "outputId": "53b5db28-bc70-4df4-ecf4-0338cef12f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/gator_sched/model_output.zip\n",
            "   creating: /content/model_output/model_output/\n",
            "   creating: /content/model_output/model_output/custom_q_and_a/\n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/training_args.bin  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/special_tokens_map.json  \n",
            "   creating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/\n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/training_args.bin  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/trainer_state.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/generation_config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/model.safetensors  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/optimizer.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/scheduler.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-20000/rng_state.pth  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/config.json  \n",
            "   creating: /content/model_output/model_output/custom_q_and_a/runs/\n",
            "   creating: /content/model_output/model_output/custom_q_and_a/runs/Dec05_20-36-04_4577102c74bc/\n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/runs/Dec05_20-36-04_4577102c74bc/events.out.tfevents.1701808572.4577102c74bc.412.0  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/generation_config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/model.safetensors  \n",
            "   creating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/\n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/training_args.bin  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/trainer_state.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/generation_config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/model.safetensors  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/optimizer.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/scheduler.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-25000/rng_state.pth  \n",
            "   creating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/\n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/training_args.bin  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/trainer_state.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/generation_config.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/model.safetensors  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/optimizer.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/scheduler.pt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/checkpoint-30000/rng_state.pth  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/merges.txt  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/vocab.json  \n",
            "  inflating: /content/model_output/model_output/custom_q_and_a/tokenizer_config.json  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the model output for usage\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/gator_sched/model_output.zip\" -d \"/content/model_output\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLfYOtZEhJ0k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EPaYL5xgjzR"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer functions\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "\n",
        "# Generate text based on model predictions\n",
        "\n",
        "def generate_text(model, tokenizer, sequence, max_length):\n",
        "    ids = tokenizer.encode(sequence, return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    return tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU4LCdD6zRXK"
      },
      "outputs": [],
      "source": [
        "# def generate_text(model_path, sequence, max_length):\n",
        "\n",
        "#     model = load_model(model_path)\n",
        "#     tokenizer = load_tokenizer(model_path)\n",
        "#     ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
        "#     final_outputs = model.generate(\n",
        "#         ids,\n",
        "#         do_sample=True,\n",
        "#         max_length=max_length,\n",
        "#         pad_token_id=model.config.eos_token_id,\n",
        "#         top_k=50,\n",
        "#         top_p=0.95,\n",
        "#     )\n",
        "#     print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vElMqB-kpg6Z"
      },
      "outputs": [],
      "source": [
        "# Read questions from a file and write answers generated by the model\n",
        "\n",
        "def read_questions(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        questions = file.readlines()\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgN1uSlIqF7P"
      },
      "outputs": [],
      "source": [
        "# def write_answers(model, tokenizer, questions, output_file_path, max_length=18):\n",
        "#     with open(output_file_path, 'w') as file:\n",
        "#         for question in questions:\n",
        "#             answer = generate_text(model, tokenizer, question.strip(), max_length)\n",
        "#             file.write(f\"Q: {question.strip()} \\n A: {answer}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JSl-JLq0Ynv"
      },
      "outputs": [],
      "source": [
        "def write_answers(model, tokenizer, questions, output_file_path, additional_length=15):\n",
        "    with open(output_file_path, 'w') as file:\n",
        "        for question in questions:\n",
        "            question_length = len(tokenizer.encode(question.strip()))\n",
        "            max_length = question_length + additional_length\n",
        "            generated_text = generate_text(model, tokenizer, question.strip(), max_length)\n",
        "\n",
        "            # Find the end of the input question in the generated text\n",
        "            question_end_index = generated_text.find(question.strip()) + len(question.strip())\n",
        "\n",
        "            # Find the start and end of the answer\n",
        "            answer_start = generated_text.find(\"Answer:\", question_end_index) + len(\"Answer:\")\n",
        "            answer_end = generated_text.find(\"Question:\", answer_start)\n",
        "\n",
        "            # Extract the answer\n",
        "            answer = generated_text[answer_start:answer_end].strip() if answer_end != -1 else generated_text[answer_start:].strip()\n",
        "\n",
        "            # Write question and answer to the file\n",
        "            file.write(f\"Q: {question.strip()} \\nA: {answer}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezSOfn5n2xJq"
      },
      "outputs": [],
      "source": [
        "# def write_answers(model, tokenizer, questions, output_file_path, additional_length=30):\n",
        "#     with open(output_file_path, 'w') as file:\n",
        "#         for question in questions:\n",
        "#             question_length = len(tokenizer.encode(question.strip()))\n",
        "#             max_length = question_length + additional_length\n",
        "#             generated_text = generate_text(model, tokenizer, question.strip(), max_length)\n",
        "\n",
        "#             # Extract the answer from the generated text\n",
        "#             answer_start = generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "#             answer_end = generated_text.find(\"Question:\", answer_start)\n",
        "#             answer = generated_text[answer_start:answer_end].strip() if answer_end != -1 else generated_text[answer_start:].strip()\n",
        "\n",
        "#             file.write(f\"Q: {question.strip()} \\nA: {answer}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmvxZlq_gl0_"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "model_path = \"/content/model_output/model_output/custom_q_and_a\"\n",
        "model = load_model(model_path)\n",
        "tokenizer = load_tokenizer(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpAPUxnjqNMm"
      },
      "outputs": [],
      "source": [
        "# Read questions and generate answers\n",
        "questions_file_path = \"/content/questions.txt\" \n",
        "output_file_path = \"/content/answers.txt\"\n",
        "questions = read_questions(questions_file_path) #generate answers using the model\n",
        "write_answers(model, tokenizer, questions, output_file_path) #save the responses in answers.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQPnoUwSzXE3"
      },
      "outputs": [],
      "source": [
        "# # model2_path = \"/content/drive/MyDrive/ColabNotebooks/models/chat_models/custom_q_and_a\"\n",
        "# sequence2 = \"Give me ideas for some new hobbies \"\n",
        "# max_len = 50\n",
        "# generate_text(model_path, sequence2, max_len)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
